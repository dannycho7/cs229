\item \subquestionpoints{2}
For linear models such as logistic regression or SVM with linear kernel, the features that we use to represent the data matter. In kernel methods (for example, SVM with RBF kernel in the part above), the features are generated by some fixed feature map $\phi(x)$. 

In the deep learning era, high-quality features $\phi(x)$ can be learned from deep learning techniques on potentially other bigger datasets. In this part, we will ask you to use these features and solve the spam classification problem better than the Naive Bayes and SVM approaches. The methods of obtaining these features are beyond the scope of this course. (In a nutshell, the feature map $\phi$ that you will use, which is called BERT,\footnote{A pretrained neural transformer model by Google. Details of the models can be found
	at \url{https://arxiv.org/abs/1810.04805}. Code and example usage can be found at 
	\url{https://github.com/huggingface/transformers}} is learned using large-scale neural networks on language modeling tasks with a huge text corpus.)

You are provided with the features $\{\phi(x^{(1)}), \dots, \phi(x^{(n)})\}$ of the training set in \url{src/spam/bert_train_matrix.tsv.bz2}, which contains a matrix $\in \mathbb{R}^{n\times 768}$ where $n$ is the number of training examples (messages). Each row is a tab separated list of floating point values representing the 768 dimensional feature for one example.  Similarly, \url{src/spam/bert_val_matrix.tsv.bz2} and \url{src/spam/bert_test_matrix.tsv.bz2} contain feature matrices for validation and test examples. The order of the examples in these feature matrices are the same as the order of the examples in the original data and labels. The code to read in the data has already been implemented for you.

A logistic regression model on top of these new features can be used to model the problem, that is,
\begin{equation}
	p(y=1 \mid x; \theta) = h_\theta(x) = \sigma(\theta^\top \phi(x))
	\label{eq:logreg_bert}
\end{equation}
where $\sigma(z)=\frac{1}{1 + \exp(-z)}$ is the sigmoid function. We have provided you the code to run gradient descent for logistic regression, and you will be asked to tune the learning rate based on the validation set accuracy.

The code to run gradient descent for logistic regression with a specific choice of learning rate is in the function \url{train_and_predict_logreg} in \url{src/spam/logreg.py}. (You should not need to modify the code in this file.) This function has 
the same input and output arguments as \url{train_and_predict_svm} in \url{src/spam/svm.py} from the previous sub-question with the exception of the last input argument being a specific learning rate instead of being a specific radius.
Please implement the function \url{compute_best_logreg_learning_rate} in \url{src/spam/spam.py} to compute and return the best learning rate for logistic regression (among the choices of learning rates provided to you in the code) that maximizes the validation accuracy, similar to the previous sub-question.

Report the best learning rate and the corresponding test set accuracy in your writeup.

\emph{Note:} It is possible that you will see some warnings about ``divide by zero" or ``invalid value" for certain learning rates. These are expected since
some of the learning rates cause gradient descent to diverge.
\\
